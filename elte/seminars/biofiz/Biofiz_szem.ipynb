{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Mismatch Negativity and the Connectivity of the Brain </center>\n",
    "\n",
    "### <center>- $\\textit{Do we predict our future?}$  - </center>\n",
    "\n",
    "\n",
    "###### <center> Kristóf Furuglyás, Eötvös Loránd University </center>\n",
    "###### <center> Biophysics Seminar, 2019 Fall </center>\n",
    "---\n",
    "\n",
    "<p>\n",
    "    <img src=\"pics/elte_cimer_szines.jpg\" alt=\"elte_log\" align=\"left\" width = 300/>\n",
    "    <img src=\"pics/wigner_emblem.png\" alt=\"wigner_log\" align=\"right\" width = 300 />\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  <center> Outline </center>\n",
    " \n",
    " <img style=\"float: right;\" src=\"pics/brainwave_artistic.png\" alt = \"table_of_cont\" width = 200>\n",
    "\n",
    "- EEG signals and brainwaves \n",
    "- Event-related potential (ERP) signals \n",
    "  <br>and mismatch negativity (MMN) \n",
    "- Predictive coding\n",
    "- The HBP-Canon project\n",
    "- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\"> EEG signals and brainwaves </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <center> Electroencephalography (EEG) </center>\n",
    "\n",
    "  <img style=\"float: right;\" src=\"pics/eeg_example.png\" alt=\"eeg_example\" width = 300>\n",
    "\n",
    "- Monitoring brain activity via <br>electric potential \n",
    "- Change in ionic currents (Na, K, Ca) \n",
    "- Non-invasive or invasive <br>(electrocorticography) \n",
    "- Main purposes: epilepsy, <br>sleeping disorders, depth of anaesthesia\n",
    "- Evoked potentials (EP), <br>event-related potentials (ERP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Electroencephalography (EEG) is an electrophysiological monitoring method to record electrical activity of the brain. Mainly used for medical purposes, however, lately it has become a tool for commercial purposes (apart from the scientific): e.g.: what are the brain's responses to different advertisements or for gaming and military uses. \n",
    "\n",
    "The change in the electric current is an overall summary of each neuron cell (or neuron populations). Most basic charasterictics of a neuron include for example summing the input voltages from its inputs then firing depending on the result of that. These tend to be in the range of millivolts and maybe hundreds of millivolts. There are several types of neurons that have different firing gates and intensities but since we cannot separate the populations only with EEG we do not care about these problems. Maybe layer-specific tasks would need this. \n",
    "\n",
    "\n",
    "Non invasive EEG ar those which do not need surgery in order to record, the sensors are based on the scalp/hair. Therefore more noise can occur. This can cover larger areas and provide a more general picture. Ivasive methods include electrocorticography where the sensors are placed directly onto the brain itself, or inside the cortex of it. Most of the times these are spikes or shanks which are good to measure layer-specific phenomenas. These provide a more precise signal (they measure a smaller population of neurons) therefore resulting in a smaller area covered (since surgery is needed most of the cases). Sensors can be as small as a penny yet still having 256 channels. \n",
    "\n",
    "\n",
    "In the field of medical use, EEG can be a benefitial to detect sleeping disorders as well as the depth of anaesthesia during a surgery but is mainly utilized when an epileptic patient needs a surgery and drugs no longer work. On the right hand side one can see the EEG recording of a child with epilepsy, during seizure. It is clear where the seizure started indicating that it was an evoked potential. However, here the cause was inside the brain.\n",
    "\n",
    "\n",
    "But many times one can create the so-called evoked potential which are time-locked to an outer stimulus, which can be visual, somatosensory on auditory. Event-related potentials are different in such a way that there is a cognitive or motor process that is being measured. These ERP-s are the result of different frequency components or to call them in the proper way, different brain waves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align = \"center\"> Brainwaves </h2>\n",
    "\n",
    "\n",
    " <img style=\"float: right;\" src=\"pics/brainwaves_freqs.png\" alt=\"eeg_example\" width = 400>\n",
    "    \n",
    "- Rhytmic neural oscillations <br>generated by the neaural tissue\n",
    "- First discovered in 1924 by Berger \n",
    "- Characterized by the frequency, <br>amplitude and phase \n",
    "- Output is the mixture of all \n",
    "- Phase synchrony \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Neural oscillations, or brainwaves, are rhythmic or repetitive patterns of neural activity in the central nervous system. Neural tissue can generate oscillatory activity in many ways, driven either by mechanisms within individual neurons or by interactions between neurons. \n",
    "\n",
    "The first human EEG recording obtained by Hans Berger in 1924. He placed a rudimentary EEG onto a person's head and after amplification the resulting signal became the so-called alpha waves (or Berger waves). Even he was capable of detecting that these brainwaves can be influenced by outer stimulus or change in the environment. \n",
    "\n",
    "The key attributes that can describe the brain activity are the frequencies, amplitudes and phases. Mainly the frequencies (and consequently their amplitudes) determine the overall potential. A large part of the frequency range can be split into multiple parts based on the physiological behaviour. These can be seen on the lef side. \n",
    "\n",
    "Going upwards one can see the Delta waves. Delta waves are low brain waves which occur during dreamless sleep and in deepest meditative states. These slow waves are not so strong and hard to achieve. Theta waves are very slow, and relate to dreamy, free-flowing, detached unconscious thought, which occurs while doing automatic tasks and sometimes in deep meditative states. It often occurs during dreaming sleep. Alpha waves are slower and higher in amplitude than Beta waves and represent a calm, relaxed state. It is the resting state of the brain, and occurs during some meditative and mindful activities. Most people can increase their Alpha waves by closing their eyes and taking a few deep breaths. The Lo-Beta waves, also known as sensorimotor rhythm (or SMR) have been shown to be very beneficial in reducing anxiety, increasing focus and overall wellbeing and health. Beta waves are the intense, focused brain activity when we are working on solving a problem or actively engaging with our environment. Hi-Beta waves are seen during highly complex, rapid thought including states of excitement and high anxiety. According to a popular theory, gamma waves may be implicated in creating the unity of conscious perception (the binding problem). These are frequencies where complex thoughts, feelings and commands are formed therefore this is the link between larger brain areas. However, there is a great amount of literature discussing not only the separation of the frequency range (including the Gamma waves also) but the direction of the communication and changes in these frequencies.\n",
    "\n",
    "All of these frequencies are present in the brain, but their amplitude what is changing through the time. Different brain areas experience different frequencies over time, for example, now that you are listening to me, your eyes process the information over and over but once you close them, after a short period of time, the signal measured at the occipital lobe (O1 or O2) would clearly show decrease in frequency, it would drop from the beta range to alpha. Same can be done with hearing. But do they have a relation? I mean, if I close my eyes, I am surely going to depend more on the sounds that I hear. Is there a significant relation between having turned off senses and turned on? We do not know -- yet.\n",
    "\n",
    "Phase differences can be great indicators of synchrony especially when it comes to the communication between larger sizes of brain areas. Together with frequencies, phases are the perfect measures of the connectivity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> ERP & MMN </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <center> Event-related potentials (ERP) </center>\n",
    "\n",
    " <img style=\"float: right;\" src=\"pics/ComponentsofERP.png\" alt=\"eeg_example\" width = 300>\n",
    "\n",
    "\n",
    "- Result of sensory, cognitive or motory event\n",
    "- Needs many trials and averaging $$\n",
    "{\\bar  x}(t)={\\frac  {1}{N}}\\sum _{{k=1}}^{N}x(t,k)=s(t)+{\\frac  {1}{N}}\\sum _{{k=1}}^{N}n(t,k)\n",
    "$$\n",
    "- P300 = Positve peak after 300 ms AS\n",
    "- Fields of occurrence include:\n",
    "    - AD/HD\n",
    "    - Parkinson's disease\n",
    "    - Multiple sclerosis\n",
    "    - Obsessive-compulsive disorder \n",
    "    - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "An event-related potential (ERP) is the measured brain response that is the direct result of a specific sensory, cognitive, or motor event. More formally, it is any stereotyped electrophysiological response to a stimulus. The study of the brain in this way provides a noninvasive means of evaluating brain functioning.  Currently, ERP is one of the most widely used methods in cognitive neuroscience research to study the physiological correlates of sensory, perceptual and cognitive activity associated with processing information.\n",
    "\n",
    "The EEG reflects thousands of simultaneously ongoing brain processes. This means that the brain response to a single stimulus or event of interest is not usually visible in the EEG recording of a single trial. To see the brain's response to a stimulus, the experimenter must conduct many trials and average the results together, causing random brain activity to be averaged out and the relevant waveform to remain, called the ERP. Furthermore, it is necessary for the noise to be a Gaussian to be uncorrelated in time.\n",
    "\n",
    "ERP waveforms consist of a series of positive and negative voltage deflections, which are related to a set of underlying components. Though some ERP components are referred to with acronyms (e.g., contingent negative variation – CNV, error-related negativity – ERN), most components are referred to by a letter (N/P) indicating polarity (negative/positive), followed by a number indicating either the latency in milliseconds or the component's ordinal position in the waveform. For instance, a negative-going peak that is the first substantial peak in the waveform and often occurs about 100 milliseconds after a stimulus is presented is often called the N100 (indicating its latency is 100 ms after the stimulus and that it is negative) or N1 (indicating that it is the first peak and is negative); it is often followed by a positive peak, usually called the P200 or P2. The stated latencies for ERP components are often quite variable, particularly so for the later components that are related to the cognitive processing of the stimulus. For example, the P300 component may exhibit a peak anywhere between 250 ms – 700 ms.\n",
    "\n",
    "It is interesting that how many functional disorders can produce ERP-s. ERP component abnormalities in clinical research have been shown in neurological conditions such as: ... So therefore the differences from a well known signal can be the indicator of abnormalities. But many times it can happen that the discrepancy is not an indicator of an irregularity but rather a somewhat surprise. This brings us to our next point, the mismatch negativity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Mismatch negativity (MMN) </center>\n",
    "\n",
    "- Component of ERP\n",
    "- Standard and deviant stimuli\n",
    "\n",
    "    [s s s s s s s s s d s s s s s s d s s s d s s s s...]\n",
    "\n",
    "- Common base is the representation of our world\n",
    "- Memory trace or no memory neurons\n",
    "\n",
    "\n",
    " <center> <img src=\"pics/mmn_example.jpg\" alt=\"mmn_example\" width = 600> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The mismatch negativity (MMN) or mismatch field (MMF) is a component of the event-related potential (ERP) to an odd stimulus in a sequence of stimuli. It can occur in any sensory system, but has most frequently been studied for hearing and for vision. In the case of auditory stimuli, the MMN occurs after an infrequent change in a repetitive sequence of sounds (sometimes the entire sequence is called an oddball sequence.) \n",
    "\n",
    "For example, a rare deviant (d) sound can be interspersed among a series of frequent standard (s) sounds (e.g., s s s s s s s s s d s s s s s s d s s s d s s s s...). The deviant sound can differ from the standards in one or more perceptual features such as pitch, duration, or loudness. The MMN is usually evoked by either a change in frequency, intensity, duration or real or apparent spatial locus of origin. The MMN can be elicited regardless of whether the subject is paying attention to the sequence. During auditory sequences, a person can be reading or watching a silent subtitled movie, yet still show a clear MMN. In the case of visual stimuli, the MMN occurs after an infrequent change in a repetitive sequence of images. \n",
    "\n",
    "The auditory MMN was discovered in 1978 by Risto Näätänen, the first report of a visual MMN was in 1990 by Rainer Cammer. On the picture below one can see the response to a deviant auditory stimulus. Presence of robust mismatch negativity responses in both patient and control groups. The auditory event-related potentials (ERPs), as well as their difference are shown. The shades indicate the SEM.\n",
    "\n",
    "If behaviourally relevant aspects of the environment are not correctly represented in the brain, then the organism's behaviour cannot be appropriate. Without these representations our ability to understand spoken language, for example, would be seriously impaired. Cognitive neuroscience has consequently emphasised the importance of understanding brain mechanisms of sensory information processing, that is, the sensory prerequisites of cognition.\n",
    " \n",
    "The mainstream \"memory trace\" interpretation of MMN is that it is elicited in response to violations of simple rules governing the properties of information. It is thought to arise from violation of an automatically formed, short-term neural model or memory trace of physical or abstract environmental regularities. However, other than MMN, there is no other neurophysiological evidence for the formation of the memory representation of those regularities. Integral to this memory trace view is that there are: i) a population of sensory afferent neuronal elements that respond to sound, and; ii) a separate population of memory neuronal elements that build a neural model of standard stimulation and respond more vigorously when the incoming stimulation violates that neural model, eliciting an MMN.\n",
    "\n",
    "When interpreting the results one might suggest that if there is  \n",
    "\n",
    "An alternative \"fresh afferent\" interpretation is that there are no memory neuronal elements, but the sensory afferent neuronal elements that are tuned to properties of the standard stimulation respond less vigorously upon repeated stimulation. Thus when a deviant activates a distinct new population of neuronal elements that is tuned to the different properties of the deviant rather than the standard, these fresh afferents respond more vigorously, eliciting an MMN. A third view is that the sensory afferents are the memory neurons. So what can cause the MMN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Predictive coding </center>\n",
    "\n",
    "\n",
    "- Brain generates hypotheses about the future\n",
    "- Repetition suppression (RS): reduction of neural response\n",
    "- Surprise enhancement (SE): prediction error\n",
    "- Ratio of the RS and SE\n",
    "- Equiprobable series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Many theories of perception are anchored in the central notion that the brain continuously updates an internal model of the world to infer the probable causes of sensory events. In this framework, the brain needs not only to predict the causes of sensory input, but also when they are most likely to happen. Predictive coding: the idea that the brain generates hypotheses about the possible causes of forthcoming sensory events and that these hypotheses are compared with incoming sensory information. The difference between top-down expectation and incoming sensory inputs, that is, prediction error, is propagated forward throughout the cortical hierarchy. \n",
    "\n",
    "Repetition suppression (RS) is a reduction of neural response that is often observed when stimuli are presented more than once. Typically, this is achieved by varying the probability of stimulus repetitions (Prep) between blocks of an experiment, generating an abstract expectation that ‘things will repeat’.\n",
    "\n",
    "Experimental data concur to suggest that gamma activity is modulated as a function of sensory surprise and, among its other possible functions, is used to signal unexpected information, that is, prediction error. Prediction error is mediated between the cortical layers in the gamma band (40-100Hz). The conflict then can be interpreted as the surprise enhancement.\n",
    "\n",
    "Also, a large number of studies detected the reduction of the neuronal response after the repetition of a given\n",
    "stimulus (repetition suppression – RS) and it was suggested that RS is the major mechanism of MMN, an explanation currently also supported by animal studies. However, human studies have proposed that a surprise-related response enhancement for the deviant stimuli might also underlie vMMN. A study by Catarina Amado and Gyula Kovács suggests that during vMMN the ratio of the might differ in humans and other animals. They found out that for vMMN the ratio also varies between the applied categories which where in this case faces, chairs, real and false characters. For faces and chairs it was largely driven by RS, whereas for real and false characters it was mainly due to surprise-related changes.\n",
    "\n",
    "So in order to compensate the RS and SE there has many attempts but unanimously the best solution was to generate a control group of responses where all the stimuli have equal probabilites. By generating equiprobable series one presents the stimuli with larger interstimuli interval (ISI) also. These can be viewed as spontaneous or pure responses. No canonical microcircuit has been \"trained\" to a given response. By comparing the last standard stimulus and the equiprobable one can also detect the RS indirectly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Short summary <center>\n",
    "    \n",
    "    \n",
    " <img style=\"float: right;\" src=\"pics/mmn_comp.png\" alt=\"mmn_comp\" width = 300>\n",
    "\n",
    "### <center> What we know </center>\n",
    "\n",
    "- Brainwaves by the frequency\n",
    "- Mismatch negativity\n",
    "- Repetiton suppression and prediction error\n",
    "- Standard, deviant and equiprobable series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "So far we have learned that based on the intesity different frequency components can be indicators of different neuronal behaviours, e.g. gamma frequency band are the information carriers between cortices. Mismatch negativity is a difference between the expected and the real input. Repetition suppression and prediction error are probably the main causes. We already know what standard, deviant and equiprobable series are. \n",
    "\n",
    "On the right hand side, you can see the connectivity between the different types of stimuli and the measure that can be derived. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    " <img style=\"float: right;\" src=\"pics/brain_sensors.png\" alt=\"mmn_comp\" width = 300>\n",
    "\n",
    "\n",
    "### <center> What we do not know </center>\n",
    "\n",
    "- Conditional MMN\n",
    "- Multiple inputs\n",
    "- Communication\n",
    "- Anaesthesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In real world experiments the sensory inputs cannot be separated and if so, if there a condition that influences outcoming signal i.e. you can only see a response to a give auditory response if and only if the animal also sees it? What happens if multiple inputs are present and how do they correlate? Does the brain shares its resources so that every response is proccessed correctly? What if we only change one of them? How do different brain areas communicate given a more complex stimulus? Do they try to build a complex picture also or just pairing them together? And one thing we barely mentioned but is interesting, do all these predictions fail if the animal is in deep sleep, in anaestheted state? Do these change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> The HBP-Canon project </center>\n",
    "\n",
    " <img style=\"float: right;\" src=\"pics/ams_uni.jpg\" alt=\"mmn_comp\" width = 300>\n",
    "\n",
    " \n",
    "\n",
    "- Swammerdam Institute for Life Sciences,<br> University of Amsterdam\n",
    "- Rats and mice, auditory and visual <br>stimuli (4 types)\n",
    "- Measured in auditory and visual <br>corteces invasively (AL, V1)\n",
    "- 2 x 32 channels, 32 kHz sampling frequency\n",
    "- Appr. 500 GB of raw recordings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is collaboration with Swammerdam Institute for Life Sciences, University of Amsterdam, Amsterdam, The Netherlands, in which we also have the opportunity to process their data, which is coming from an experiment. The purpose if this research is to investigate the underlying mechanisms of the brain during bimodal stimulation. Rats' and mice's brain activities have been measured whilst they have been exposed to two types of stimuli resulting in 4 different outcomes altogether. Data acquisition was done by 64 channels with 32 kHz sampling frequency therefore we have access to half a terabyte of raw LFP data, since this setup was done approximately 20 times with different lengths of recording time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> <img src=\"pics/stim_types.png\" alt=\"mmn_comp\" width = 500> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Stimuli details\n",
    "\n",
    "Duration: 500 ms (start at t=0)\n",
    "\n",
    "ITI: 1.5 s (12 s for deviant only control)\n",
    "\n",
    "Visual stimuli:\n",
    "    Sinusoidal drifting grating\n",
    "    Spatial freq: 0.05 c/deg\n",
    "    Temporal freq: 2 Hz\n",
    "    Contrast: 50%\n",
    "    (Static grating during ITI)\n",
    "\n",
    "Auditory stimuli:\n",
    "    Band-limited white noise\n",
    "    Volume: ~75dB\n",
    "    Low-pitch: 8-12 kHz\n",
    "    High-pitch: 12-16 kHz\n",
    "    (Silence during ITI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Results </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <center> Raw average of Ab (V1) </center>\n",
    "<center> <img src=\"pics/A_standardSTIM_b_V1.png\" alt=\"mmn_comp\" width = 700> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This pic is the average raw response given to a B stimulus when A was the standard -- see the notation above. As I've mentioned, all in all there were 64 channels, 32-32 both in the V1 and AL cortex -- 8 channel on 4 shanks. The numbers at left side mean the channel number, which are not in order. X axis means time in millisec, y axis LFP or voltage in microsec. The yellow markers mean the start and the end of the stimulus. What is noticable here is the layer-specificity. Sensors in the same layer tend to measure similar responses. Furthermore, there are small valley before the start of the stimulus, which are either a prediction for the next or late-response to the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Raw average of Ab (AL) </center>\n",
    "<center> <img src=\"pics/A_standardSTIM_b_AL.png\" alt=\"mmn_comp\" width = 700> </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The same applies here, only difference is that these recordings are from the auditory cortex. Layer-specificity here is visible, too. Now let us compare the results for different stimuli, on one channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Comparison of responses (ch1 from V1) </center>\n",
    "<center> <img src=\"pics/MMN_1_pre.png\" alt=\"mmn_comp\" width = 700> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "I choose two arbitrary channels, one from the visual and one from the auditory (1 and 45, respectively). Here you can see the average responses to different stimuli with having different standards. For example, lets take STIM_a. There you can see that the black line means that A was the standard also, therefore having less noisy signal because the number of standards that have been averaged have a cardinality nearly 20 times as large as the deviants'. Noteable that there is a on/off switch response just after the start and the end of the stimulus, which are the by-products of the electronics. Well, the red line shows the same tendency (C standard), whilst the the other two are similar to each other in amplitudes also. Notice that the range between +- 4000 microvolts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Comparison of responses (ch45 from AL) </center>\n",
    "<center> <img src=\"pics/MMN_45_pre.png\" alt=\"mmn_comp\" width = 700> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here you can see something similar, except that the range here is now up to 15 thousand. Since I always forget which stimulus is which, it is better co compare the responses by their mismatches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> MMN types (ch1 from V1) </center>\n",
    "<center> <img src=\"pics/MMN_1_pro.png\" alt=\"mmn_comp\" width = 700> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Taking a channel from the visual cortex, one can see that the standards only have slight changes in their respones and even vmmn is barely seen. The reason behind this is something that I did not tell you beforehand. This animal was anaesthatised state. Therefore it is possible to think that the brain automatically swithes off its eyesight. Interesting that auditory mismatch does produce a higher response, so it is not surprising that during bimodal mismatch the visual cortex generates a similar response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> MMN types (ch45 from AL) </center>\n",
    "<center> <img src=\"pics/MMN_45_pro.png\" alt=\"mmn_comp\" width = 700> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In anaesthatised state, the hearing is still active. However, similar responses are also present for standards and visual mismatch does not play a crucial role. BUT. For auditory mismatch there are two significantly larger spikes for the Cd and Dc responses. These are the ones where the visual stimulus was vertical grating. This is the thing that we can call conditional mismatch negativity. None of us from the group found evidence of a study published about this phenomena. The other two signals are also larger -- see the scales -- but a significance test would necessary to prove it mathematically also. Similar tendency can be seen for bimodal mismatch, but testing this for other channels would give us more information also. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center> Sidenote </center>\n",
    "\n",
    "### <center> Coherency </center>\n",
    "\n",
    "The cross-spectrum $(S_{xy})$ of two temporal signals $(x,y)$ is the following :\n",
    "$$\n",
    "\\left< S_{xy} \\left( f \\right) \\right> = \\frac{2\\Delta^2}{T} \\frac{1}{K} \\sum_{k=1}^K F_x^k \\left(f\\right) F_y^{k,*}\\left(f\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\Delta$ : timestep ($dt$)\n",
    "- $T$: length of a trial\n",
    "- $K$ : trials\n",
    "- $F_x^k$: Fourier spectrum\n",
    "\n",
    "\n",
    "and the coherency:\n",
    "\n",
    "$$\n",
    "\\kappa_{xy} \\left( f\\right) = \\frac{\\left| \\left< S_{xy}\\left(f\\right) \\right> \\right|}{\\sqrt{\\left< S_{xx}\\left(f\\right)\\right>\\left< S_{yy}\\left(f\\right)\\right>}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "So far, we only talked about the pure temporal feedbacks of the channels. To investigate the connectivity further, let me introduce the coherency. In physics, two wave sources are perfectly coherent if they have a constant phase difference and the same frequency, and the same waveform.  Coherence describes the statistical similarity of a field (electromagnetic field, quantum wave packet etc.) at two points in space or time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Total coherency map </center>\n",
    "<center> <img src=\"pics/cohmap_Ac_12_30.png\" alt=\"mmn_comp\" width = 1200> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Above you can see the total coherency map of all the channels for Ac between 12 and 30 Hz. So, responses within one cortex are more similar to each other, and there are rows which are somewhat breaking the coherency in coloumns. This strutcure is the reason of the identification of the channels. As I've said, the channels were not placed in order and ch no1. was closer to ch no9. than to - for example - ch no6 -- which cannot be seen here. Now let us take the average for the cortices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Coherency-difference averaged map </center>\n",
    "<center> <img src=\"pics/diff_M_22_45.png\" alt=\"mmn_comp\" width = 1200> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This picture shows a chessboard-table strucutre, which is an indicator that mismatches are the symmetric in a sense. The matrix shows the difference in coherence for each stimulus between its deviant and standard presence. Here one can see comparable difference in the cross-coherency between the two cortices also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Coherency-difference MMN map of V1 </center>\n",
    "<center> \n",
    "\n",
    "<a href=\"pics/coh_final_v1.png\" target=\"_blank\">\n",
    " <img src=\"pics/coh_final_v1.png\" alt=\"mmn_comp\" width = 600>\n",
    "</a>\n",
    "\n",
    " </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "These are the latest result to discuss. The second coloumn figures are those which were sampled just before the stimulus [-490, -10]. This is another controlling sample.  Here the main problem is that it would really need a t-test for significancy. Subtitles are the mismatch types and the bars are the comparison mostly from the triangle I showed you before. The other measures are for the only first and only last standards. RS and PE are under construction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Coherency-difference MMN map of AL </center>\n",
    "<center> \n",
    "\n",
    "<a href=\"pics/coh_final_al.png\" target=\"_blank\">\n",
    " <img src=\"pics/coh_final_al.png\" alt=\"mmn_comp\" width = 600>\n",
    "</a>\n",
    "\n",
    " </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here, you can see that bimodal and ammn are similar also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Crosss-coherency-difference MMN map of V1 and AL </center>\n",
    "<center> \n",
    "\n",
    "<a href=\"pics/coh_final_cross.png\" target=\"_blank\">\n",
    " <img src=\"pics/coh_final_cross.png\" alt=\"mmn_comp\" width = 600>\n",
    "</a>\n",
    "\n",
    " </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is interesting that the cross coherency increased for the lower frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> #TODO </center>\n",
    "\n",
    "- Group by other paramteres (freq, mmn, type, region)\n",
    "- Significance test\n",
    "- Compare to Fourier spectrum\n",
    "- Do for all possible experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Our group </center>\n",
    "\n",
    "- Zsigmond Benkő: MTA junior researcher, Phd candidate\n",
    "- László Négyessy: Senior Research Fellow \n",
    "- Zoltán Somogyvári: Senior Research Fellow \n",
    "\n",
    " <img style=\"float: left;\" src=\"pics/zsiga.png\" alt=\"mmn_comp\" width = 100>\n",
    " <img style=\"float: right;\" src=\"pics/soma.jpg\" alt=\"mmn_comp\" width = 100>\n",
    "<center> <img src=\"pics/nlaci.jpg\" alt=\"mmn_comp\" width = 100> </center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1> <center> References </center> </h1>\n",
    "\n",
    "<ul>\n",
    "    <li> Iyer, Parameswaran Mahadeva, et al. \"Mismatch negativity as an indicator of cognitive sub-domain dysfunction in amyotrophic lateral sclerosis.\" Frontiers in neurology 8 (2017): 395.</li>\n",
    "    <li> Näätänen, Risto, et al. \"The mismatch negativity (MMN) in basic research of central auditory processing: a review.\" Clinical neurophysiology 118.12 (2007): 2544-2590. </li>\n",
    "    <li> Amado, Catarina, and Gyula Kovács. \"Does surprise enhancement or repetition suppression explain visual mismatch negativity?.\" European Journal of Neuroscience 43.12 (2016): 1590-1600. </li>\n",
    "    <li> Epstein, Russell A., Whitney E. Parker, and Alana M. Feiler. \"Two kinds of fMRI repetition suppression? Evidence for dissociable neural mechanisms.\" Journal of Neurophysiology 99.6 (2008): 2877-2886.</li>\n",
    "    <li> Arnal, Luc H., and Anne-Lise Giraud. \"Cortical oscillations and sensory predictions.\" Trends in cognitive sciences 16.7 (2012): 390-398.</li>\n",
    "    <li> Bastos, Andre M., et al. \"Canonical microcircuits for predictive coding.\" Neuron 76.4 (2012): 695-711.</li>\n",
    "    <li> https://www.myndlift.com/single-post/2018/01/23/How-Does-Our-Brain-Work-1 </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### <center> Kristóf Furuglyás, Eötvös Loránd University </center>\n",
    "###### <center> Biophysics Seminar, 2019 Fall </center>\n",
    "\n",
    "\n",
    "# <center> Thank you for your attention! </center>\n",
    "\n",
    "---\n",
    "\n",
    "<p>\n",
    "    <img src=\"pics/elte_cimer_szines.jpg\" alt=\"elte_log\" align=\"left\" width = 200/>\n",
    "    <img src=\"pics/wigner_emblem.png\" alt=\"wigner_log\" align=\"right\" width = 200 />\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
